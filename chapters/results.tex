\chapter{Results}

In this chapter, the results are exposed in the following way: Section \ref{dset_section} shows how the dataset has been created and the overall proportion of classes. Section \ref{mono} is reserved to comparison and evaluation of monolithic machine learning models, Decision Tree, K-Nearest Neighbors, Multi Layer Perceptron, Naive Bayes and Support Vector Machine. For Multiple Classifier System (MCS), the Section \ref{mcs} will compare the KNORAE, KNORAU, META-DES, OLA, Single Best and Static Selection. After the comparison between Monolithic and MCS, the overall comparison is going to be discussed in section \ref{result_ending}.

Using the dataset described in Section \ref{dset_section} other 30 different datasets have been randomly sampled that are subdivided into train and test, containing 80\% of the original dataset for model training and 20\% to test the model effectiveness. This method intends to make a Hypothesis Test, this help to statically evaluate if a classifier is really effective when applied in this dataset. After the classifiers applied to each of the 30 datasets, the mean and standard deviation of each metric is calculated and displayed in the result tables. For each classification method, both Monolithic and MCS, will have its setup described in Section \ref{mono} and \ref{mcs} to ease future replications of this work. 

Each classifier is evaluated with 4 different metrics, precision (equation \ref{precision}), recall (equation \ref{recall}), F1 score (equation \ref{f1}) and accuracy (equation \ref{accuracy}). The precision is the ratio of correctly predictions to the total predictions done. Accuracy is the ratio of correctly true predictions to the total of predictions. Recall is the ratio of correctly positive predictions to all the predictions of a class. F1 score represents the harmonic mean between precision and recall, which is a more meaningful metric than the mean between precision and recall \cite{sasaki2007truth}. True positives are all instances of a class $C$ that are correctly classified. True negatives are all instances that do not belong to $C$ and are correctly classified. False positives of a class $C$ are the instances of other classes that has been classified as $C$. False negatives are instances of $C$ that has been classified as not belonging to $C$.

\begin{multicols}{2}%
    \noindent%
    {%
    \begin{equation} \label{precision} precision = \frac{TP}{TP+FP} \end{equation}%
    \begin{equation} \label{recall} recall = \frac{TP}{TP+FN} \end{equation}%
    }%
    {%
    \begin{equation} \label{f1} F1 = \frac{2 \times recall \times precision}{recall + precision} \end{equation}%
    \begin{equation} \label{accuracy} accuracy = \frac{TP+TN}{TP+TN+FP+FN} \end{equation}%
    }%
\end{multicols}

\section{Dataset}\label{dset_section}

The dataset created in this work uses the feature extractor developed by \cite{rasthofer2014machine}. The extractor creates meaningful information using the Android methods names and their real implementation in the Android API. As result, the extractor gives the method class, if has been hand annotated, and a list of features. These features are lexical, semantic and syntactic, containing information about the method name, parameters, return type, method and classes modifiers, class modifiers, if exists data flow in the method return or parameters and the required permissions.

Lexical features follows the idea that the Android APIs adopt a specific coding style, described in \cite{androidcoderef}, extracting if a method name or parameter name contains certain strings can lead to the prediction of the method class. For syntactic features, the feature extractor evaluates if exists data flow inside a method using Taint Analysis, already discussed in Chapter \ref{background}. Finally, the semantic features are information extracted from classes modifiers, such as private, public, protected and types of variables, arguments and methods returns.

As result, the feature extractor gives 215 semantic, syntactic and lexical features extracted from the Android API Level 3 to Level 27, excluding APIs Level 18 and 20 which complete binaries could not be found. The features extracted consists in boolean variables represented in 12 categories, the dataset also contains the full method name and signature but is not being used in classification. The lexical features are extracted by analyzing the stream of characters from the source code, syntactic features represents the dependency of data and control for code variables and methods, semantic features consists in the types and modifiers for variables, methods and classes \cite{aho2003compilers}.

\begin{table}[hb!]
    \centering
    \renewcommand{\arraystretch}{1.8}
    \begin{tabular}{ p{3cm}p{2.5cm}p{2.5cm}p{2.5cm}p{2cm} }
        \toprule
        API & Source & Sink & Neithernor & Total\\
        \midrule
        API Level 17 & 316 (59.07) & 131 (24.49) & 88 (16.45) & 535 (100)\\
        Dropped Names & 322 (59.30) & 134 (24.68) & 87 (16.02) & 543 (100)\\
        Dropped Entries & 375 (55.97) & 176 (26.27) & 119 (17.76) & 670 (100)\\ [1ex]
        \bottomrule
        \end{tabular}
        \caption{%
        Overview of the generated dataset, Support(Percentage). First, the proportion for API Level 17, second, the dataset without duplicated method names (Dropped Names) and finally, the final dataset without duplicated entries (Dropped Entries).%
        }\label{dset_prop}
\end{table}

With that said, the Table \ref{dset_prop} shows the proportion of each class for the dataset, first we evaluate the feature extractor using the Android API 17, same API used in \cite{rasthofer2014machine} and we could extract a total of 535 methods, consisting in 131 sinks, 88 sources and 316 neither nor. Using all the APIs found, we extracted 670 methods in total when dropping repeated entries (Dropped Entries dataset), being 176 sinks, 119 sources and 375 neither nor. It is important to observe that none of the dataset entries are duplicated, but if we look closely to the dataset, the same method can have different value of features through different APIs, as shown if Figure \ref{menthod_though_apis}. So, considering method names as duplication factor (Dropped Names dataset), we end up with only 543 method, unlike the 670 for the Dropped Entries, disperse in 134 sinks, 87 sources and 322 neither nor. As different feature values consists in different entries, despite the same method name, we considered the larger dataset in order to have a bigger quantity of data to be analyzed, so we only dropped entries that are really duplicated. Sometimes the feature extractor could not infer syntactic features for a method, these entries are also removed from the dataset to keep the dataset integrity.

As default dataset for the evaluation, this work will use the dataset with dropped repeated entries, this dataset have more information to be learned and already guarantee the division in train and test to be disjoint as have no repetition.

\begin{figure}[h!]
    \centering
    \renewcommand{\arraystretch}{1.8}
    \includegraphics[width=0.9\linewidth]{images/menthod_though_apis.png}
    \caption{%
    Methods through different API Levels. We can observe in yellow the feature "Method modifier is PUBLIC" and the full method name. Even for different features values, marked in green, the method still belongs to the sink class.%
    }
\end{figure}\label{menthod_though_apis}

\input{chapters/results/monolithic}
\input{chapters/results/mcs}
\input{chapters/results/final_considerations}