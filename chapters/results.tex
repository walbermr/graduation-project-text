\chapter{Results}\label{chapter:results}

In this chapter, the results are exposed in the following way: Section \ref{dset_section} shows how the dataset has been created and the overall proportion of classes. Section \ref{mono} is reserved to comparison and evaluation of monolithic machine learning models, Decision Tree, K-Nearest Neighbors, Multi Layer Perceptron, Naive Bayes and Support Vector Machine. For Multiple Classifier System (MCS), the Section \ref{mcs} compares the KNORAE, KNORAU, META-DES, OLA, Single Best and Static Selection. After the comparison between Monolithic and MCS, the overall comparison is discussed in Section \ref{result_ending}.

Using the dataset described in Section \ref{dset_section} other 30 different datasets have been randomly sampled that are subdivided into two: 80\% of the original dataset for model training and 20\% to test the model effectiveness. This method intends to make a Hypothesis Test, this help to statically evaluate if a classifier is really effective when applied in this dataset. After the classifiers applied to each of the 30 datasets, the mean and standard deviation of each metric are calculated and displayed in the result tables. For each classification method, both Monolithic and MCS have its setup described in Sections \ref{mono} and \ref{mcs} to ease future replications of this work. 

Each classifier is evaluated with 4 different metrics, precision (equation \ref{precision}), recall (equation \ref{recall}), F1 score (equation \ref{f1}) and accuracy (equation \ref{accuracy}), the classifier with bigger values in these metrics have the best result, they vary from 0 to 1. The precision is the ratio of correctly predictions to the total predictions done. Accuracy is the ratio of correctly true predictions to the total of predictions. Recall is the ratio of correctly positive predictions to all the predictions of a class. F1 score represents the harmonic mean between precision and recall, which is a more meaningful metric than the mean between precision and recall \cite{sasaki2007truth}. True positives are all instances of a class $C$ that are correctly classified. True negatives are all instances that do not belong to $C$ and are correctly classified. False positives of a class $C$ are the instances of other classes that has been classified as $C$. False negatives are instances of $C$ that has been classified as not belonging to $C$.

\begin{multicols}{2}%
    \noindent%
    {%
    \begin{equation} \label{precision} precision = \frac{TP}{TP+FP} \end{equation}%
    \begin{equation} \label{recall} recall = \frac{TP}{TP+FN} \end{equation}%
    }%
    {%
    \begin{equation} \label{f1} F1 = \frac{2 \times recall \times precision}{recall + precision} \end{equation}%
    \begin{equation} \label{accuracy} accuracy = \frac{TP+TN}{TP+TN+FP+FN} \end{equation}%
    }%
\end{multicols}

\section{Dataset}\label{dset_section}

The dataset created in this work uses the feature extractor developed by \cite{rasthofer2014machine}. The extractor creates meaningful information using the Android methods names and their real implementation in the Android API. As result, the extractor gives the method class, if has been hand annotated, and a list of features. These features are lexical, semantic and syntactic, which contains information about the method name, parameters, return type, method modifiers, classes modifiers, if exists data flow in the method return or parameters and the required permissions.

Lexical features follows the idea that the Android APIs adopts a specific coding style, described in \cite{androidcoderef}. Extracting if a method name or parameter name contains certain strings can lead to the prediction of the method class. For syntactic features, the feature extractor evaluates if exists data flow inside a method using Taint Analysis, discussed in Chapter \ref{chapter:background}. Finally, the semantic features are information extracted from classes modifiers, such as private, public, protected and types of variables, arguments and methods returns.

As result, the feature extractor gives 215 features, consisting in 53 semantic, 45 syntactic and 117 lexical features, extracted from the Android API Level 3 to Level 27, excluding APIs Level 18 and 20 which complete binaries could not be found. The features extracted consists in boolean variables represented in 12 categories, the dataset also contains the full method name and signature but is not being used in classification. The lexical features are extracted by analyzing the stream of characters from the source code, syntactic features represents the dependency of data and control for code variables and methods, semantic features consists in the types and modifiers for variables, methods and classes \cite{aho2003compilers}.

\begin{table}[hb!]
    \centering
    \renewcommand{\arraystretch}{1.8}
    \begin{tabular}{p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm} }
        \toprule
        Source & Sink & Neithernor & Total\\
        \midrule
        375 (55.97 \%) & 176 (26.27 \%) & 119 (17.76 \%) & 670 (100 \%)\\ [1ex]
        \bottomrule
        \end{tabular}
        \caption{%
        The final dataset proportion without duplicated entries.%
        }\label{dset_prop}
\end{table}

With that said, Table \ref{dset_prop} shows the proportion of each class for the dataset. First we evaluate the feature extractor using the Android API 17, same API used in \cite{rasthofer2014machine} and we could extract a total of 535 methods, consisting in 131 sinks, 88 sources and 316 neither nor. Using Android API Level 3 to Level 27, excluding APIs Level 18 and 20, we extracted 670 methods in total when dropping repeated entries (Dropped Entries dataset), being 176 sinks, 119 sources and 375 neither nor. It is important to observe that none of the dataset entries are duplicated, but if we look closely to the dataset, the same method can have different value of features through different APIs, as shown in Figure \ref{menthod_though_apis}. So, considering method names as duplication factor (Dropped Names dataset), we end up with only 543 method, unlike the 670 for the Dropped Entries, disperse in 134 sinks, 87 sources and 322 neithernor. As different feature values consists in different entries, despite the same method name, we considered the larger dataset in order to have a bigger quantity of data to be analyzed, so we only dropped entries that are really duplicated. Sometimes the feature extractor could not infer syntactic features for a method, these entries are also removed from the dataset to keep the dataset integrity.

Due to the repetition of methods in different APIs, and not necessarily the same features repeated, drop entries with repeated method names reduces the quantity of methods available to be trained. So we chose as default dataset for the evaluation the dataset with dropped repeated entries, this dataset have more information to be learned and already guarantee the division in training and testing to be disjoint as have no repetition.

\begin{figure}[h!]
    \centering
    \renewcommand{\arraystretch}{1.8}
    \includegraphics[width=0.9\linewidth]{images/menthod_though_apis.png}
    \caption{%
    Methods through different API Levels. We can observe in yellow the feature "Method modifier is PUBLIC" and the full method name. Even for different features values, marked in green, the method still belongs to the sink class.%
    }
\end{figure}\label{menthod_though_apis}

\input{chapters/results/monolithic}
\input{chapters/results/mcs}
\input{chapters/results/final_considerations}