\section{Monolithic}\label{mono}

For the Monolithic classifiers, we have the Decision Tree, discussed in section \ref{decisiontree_section}, Multinomial and Bernoulli Naive Bayes \ref{naive_bayes_section}, K-Nearest Neighbor (KNN) discussed in section \ref{knn_section}, Support Vector Machine (SVM) \ref{svm_section} and Random Forest \ref{rf_section}.

The setup for Decision Tree is the split strategy to select the most relevant feature with Gini Impurity as criterion. In Multinomial and Bernoulli Naive Bayes we choose a $\alpha$ of 1. For KNN, we use 5 as the number of neighbors and a uniform weight for the neighbor points. For Perceptron we used no regularization, no early stopping, max of 1000 iterations and tolerance of $10^{-1}$.

\begin{table}[h!]
    \centering
    \renewcommand{\arraystretch}{1.8}
    \begin{tabular}{ |p{3cm}|p{2.6cm}p{2.6cm}p{2.6cm}p{2.6cm}| }
        \hline
        Model & Precision & Recall & F1 Score & Accuracy \\
        \hline
        Decision Tree & 0.8474(0.0505) & 0.8388(0.0549) & 0.8410(0.0357) & 0.8497(0.0217) \\
        Multinomial NB & 0.8297(0.0595) & 0.8211(0.0552) & 0.8231(0.0416) & 0.8387(0.0282) \\
        Bernoulli NB & 0.8307(0.0609) & 0.8180(0.0561) & 0.8220(0.0424) & 0.8378(0.0271) \\
        KNN & 0.8635(0.0547) & 0.7991(0.0657) & 0.8217(0.0484) & 0.8432(0.0307) \\
        Linear SVM & 0.8920(0.0559) & 0.8718(0.0539) & 0.8796(0.0436) & 0.8859(0.0284) \\
        MLP & 0.8936(0.0547) & \textbf{0.8759(0.0541)} & \textbf{0.8830(0.0437)} & \textbf{0.8908(0.0302)} \\
        Random Forest & \textbf{0.8953(0.0568)} & 0.8522(0.0605) & 0.8684(0.0438) & 0.8791(0.0309) \\ [1ex]
        \hline
        \end{tabular}
        \caption{%
        Result for Monolithic Classifiers, Mean(Standard Deviation), the best classifier for each metric is highlighted in bold.%
        }\label{mcs_perceptron_table}
\end{table}

We can observe that the     