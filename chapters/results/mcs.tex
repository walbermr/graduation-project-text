\section{Multiple Classifier System}\label{mcs}

For the Multiple Classifier System we used KNORAE, KNORAU, META-DES, OLA, Single Best and Static Selection. Each of these algorithms has the objective to select the best classifier in a set, in our evaluation, we are using a pool of 100 classifiers of the same base class.

The base classifier classes used are Perceptron and Decision Tree, in all ensemble algorithms we use the same base classifier configuration. For Perceptron we used no regularization, no early stopping, max of 1000 iterations and tolerance of $10^{-1}$. For Decision Tree, the split strategy is to select the most relevant feature with Gini Impurity as criterion. For ensemble algorithms, the KNORAE and KNORAU parameters were KNN to estimate the classifier competence using 7 neighbors, with no dynamic pruning and no indecision region. For META-DES, we are using Multinomial Naive Bayes as meta-classifier, 5 output profiles to estimate the competence using a KNN with 7 neighbors to decide the region of competence. And finally, static selection, we are choosing 50\% of the base classifiers. All parameters can be seen in Table \ref{table:mcs_params}.

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.8}
    \begin{tabular}{ p{5cm}p{3cm}|p{3cm}|p{3cm} }
        \toprule
        Model & \multicolumn{3}{c}{Parameters} \\
        \midrule
        Perceptron & max iterations \newline 1000 & tolerance $10^{-4}$ & \\
        Decision Tree & criterion Gini & spliter best & \\
        \midrule
        KNORAE and KNORAU & $k = 7$ & no prune & no indecision \\
        META-DES & Multinom. NB & $K_p = 5$ & $k = 7$ \\
        Static Selection & selection 50\% & & \\
        \bottomrule
        \end{tabular}
        \caption{%
        Parameters for Multiple Classifier System%
        }\label{table:mcs_params}
        \vspace{4ex}
\end{table}

In Table \ref{mcs_dt_table} is presented the MCS algorithms using Decision Tree classifier. We can observe a better mean result using META-DES in all metrics, except when comparing the Accuracy with KNORAE. Considering the mean and standard deviation, the results for KNORAE, KNORAU, OLA, Static Selection and META-DES are very close to the same interval. But we can observe that there is a bigger gap between OLA and Static Selection, and KNORAE, KNORAU and META-DES in Precision, Recall and F1. Comparing the Accuracy there is a smaller gap.

\begin{table}[!t]
    \centering
    \renewcommand{\arraystretch}{1.8}
    \begin{tabular}{ p{3cm}p{2.8cm}p{2.8cm}p{2.8cm}p{2.8cm} }
        \toprule
        Model & Precision & Recall & F1 Score & Accuracy \\
        \midrule
        KNORAE &            0.8602 (0.0517) & 0.8395 (0.0656) & 0.8469 (0.0455) & \textbf{0.8580 (0.0306)} \\
        KNORAU &            0.8377 (0.0557) & 0.8071 (0.0743) & 0.8181 (0.0495) & 0.8313 (0.0321) \\
        META-DES &          \textbf{0.8609 (0.0551)} & \textbf{0.8423 (0.0639)} & \textbf{0.8492 (0.0482)} & 0.8575 (0.0344) \\
        OLA &               0.8263 (0.0589) & 0.8104 (0.0667) & 0.8158 (0.0532) & 0.8306 (0.0387) \\
        Single Best &       0.7609 (0.0739) & 0.7356 (0.0994) & 0.7423 (0.0688) & 0.7629 (0.0447) \\
        Static Selection &  0.8424 (0.0579) & 0.8099 (0.0741) & 0.8213 (0.0492) & 0.8338 (0.0341) \\ [1ex]
        \bottomrule
        \end{tabular}
        \caption{%
        Result for Multiple Classifier System, Mean (Standard Deviation), using Decision Tree as main classifier with most relevant feature as split strategy and Gini Impurity as criterion. The best classifier for each metric is highlighted in bold%
        }\label{mcs_dt_table}
        \vspace{4ex}
\end{table}

\begin{table}[!t]
    \centering
    \renewcommand{\arraystretch}{1.8}
    \begin{tabular}{ p{3cm}p{2.8cm}p{2.8cm}p{2.8cm}p{2.8cm} }
        \toprule
        Model & Precision & Recall & F1 Score & Accuracy \\
        \midrule
        KNORAE &            0.8607 (0.0556) & \textbf{0.8368 (0.0678)} & \textbf{0.8452 (0.0471)} & \textbf{0.8570 (0.0273)} \\
        KNORAU &            0.8446 (0.0562) & 0.8148 (0.0581) & 0.8260 (0.0451) & 0.8386 (0.0301) \\
        META-DES &          \textbf{0.8748 (0.0559)} & 0.8167 (0.0700) & 0.8375 (0.0518) & 0.8525 (0.0335) \\
        OLA &               0.8418 (0.0580) & 0.8216 (0.0550) & 0.8282 (0.0413) & 0.8410 (0.0268) \\
        Single Best &       0.7913 (0.0753) & 0.7552 (0.1007) & 0.7644 (0.0688) & 0.7866 (0.0451) \\
        Static Selection &  0.8523 (0.0593) & 0.8158 (0.0717) & 0.8291 (0.0512) & 0.8438 (0.0329) \\ [1ex]
        \bottomrule
        \end{tabular}
        \caption{%
        Result for Multiple Classifier System, Mean (Standard Deviation), using Perceptron as main classifier using no regularization, no early stopping, max of 1000 iterations and tolerance of $10^{-1}$. The best classifier for each metric is highlighted in bold%
        }\label{mcs_perceptron_table}
        \vspace{4ex}
\end{table}

When using Perceptron as main classifier, Table \ref{mcs_perceptron_table}, we can observe that KNORAE have the best mean results in Recall, F1 and Accuracy but META-DES has a better Precision. When considering mean and standard deviation, the results of KNORAE, KNORAU, META-DES, Static Selection and OLA are in the same interval for all the metrics. The difference between KNORAE and META-DES is smaller in Precision, F1 and Accuracy but has a smaller Recall comparing to OLA. 

The results for best classifier using Decision Tree has close results to the best classifier using Perceptron. Comparing KNORAE with Perceptron and META-DES with Decision Tree, the META-DES with Decision Tree has better results in all metrics, but again the difference between both is so small that do not exceed the confidence interval.
